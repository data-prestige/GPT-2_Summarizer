{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a44c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import add_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6872834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT21024Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, ids_file, mode='train',length=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = add_special_tokens()\n",
    "\n",
    "        # with open(ids_file,'r') as f:\n",
    "            # if mode=='train':\n",
    "            #     self.idxs = np.array(json.load(f)['train_ids'])\n",
    "            # elif mode=='valid':\n",
    "            #     self.idxs = np.array(json.load(f)['valid_ids'])\n",
    "            # elif mode=='test':\n",
    "            #     self.idxs = np.array(json.load(f)['test_ids'])\n",
    "\n",
    "            # self.idxs = self.idxs -min(self.idxs)\n",
    "        \n",
    "        self.idxs = os.listdir(root_dir)\n",
    "        self.mode = mode\n",
    "        if len == None:\n",
    "            self.len = len(self.idxs)\n",
    "        else:\n",
    "            self.len = length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        if self.mode=='valid':\n",
    "            idx = self.idxs[-idx]\n",
    "        elif self.mode=='test':\n",
    "            idx = self.idxs[-idx-self.len]   # assuming valid and test set of same sizes\n",
    "        else:\n",
    "            idx = self.idxs[idx]\n",
    "        # file_name = os.path.join(self.root_dir,str(idx)+\".json\")\n",
    "        file_name = os.path.join(self.root_dir,str(idx))\n",
    "        with open(file_name,'r') as f:\n",
    "              data = json.load(f)\n",
    "        text = self.tokenizer.encode(self.tokenizer.pad_token)*1024\n",
    "        content = data['article'] + self.tokenizer.encode(self.tokenizer.sep_token) + data['abstract']\n",
    "        text[:len(content)] = content\n",
    "        text = torch.tensor(text)\n",
    "        sample = {'article': text, 'sum_idx': len(data['article'])}\n",
    "        return sample"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
