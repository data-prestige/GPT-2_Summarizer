{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6943020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2893881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import add_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba420a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "dm_single_close_quote = '\\u2019' # unicode\n",
    "dm_double_close_quote = '\\u201d'\n",
    "# acceptable ways to end a sentence\n",
    "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"',\n",
    "              dm_single_close_quote, dm_double_close_quote, \")\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f358f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_missing_period(line):\n",
    "    \"\"\"Adds a period to a line that is missing a period\"\"\"\n",
    "    if \"@highlight\" in line:\n",
    "        return line\n",
    "    if line == \"\":\n",
    "        return line\n",
    "    if line[-1] in END_TOKENS:\n",
    "        return line\n",
    "    return line + \" .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d231493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_art_abs(lines):\n",
    "    \"\"\" return as list of sentences\"\"\"\n",
    "\n",
    "    # truncated trailing spaces, and normalize spaces\n",
    "    lines = [' '.join(line.strip().split()) for line in lines]\n",
    "    lines = [fix_missing_period(line) for line in lines]\n",
    "\n",
    "    # Separate out article and abstract sentences\n",
    "    article_lines = []\n",
    "    highlights = []\n",
    "    next_is_highlight = False\n",
    "    for idx, line in enumerate(lines):\n",
    "        if line == \"\":\n",
    "            continue # empty line\n",
    "        elif line.startswith(\"@highlight\"):\n",
    "            next_is_highlight = True\n",
    "        elif next_is_highlight:\n",
    "            highlights.append(line)\n",
    "        else:\n",
    "            article_lines.append(line)\n",
    "    return ' '.join(article_lines), ' '.join(highlights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b0f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(i,article, abstract):\n",
    "\t\"\"\" Saves a json file.\"\"\"\n",
    "\n",
    "\tfile = \"./gpt2_1024_data/\"+str(i)+\".json\"\n",
    "\tjs_example = {}\n",
    "\tjs_example['id'] = i\n",
    "\tjs_example['article'] = article\n",
    "\tjs_example['abstract'] = abstract\n",
    "\twith open(file, 'w') as f:\n",
    "\t\tjson.dump(js_example, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9cda17",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def main(file_names, directory):\n",
    "\t\"\"\" Reads txt files, extract articles and summaries, tokenize them and save as json files\n",
    "\t\tArgs:\n",
    "\t\t\tfile_names: list, all the articles with total no of tokens less than 1024\n",
    "\t\t\tdirectory: string, directory where files in file_names is stored\n",
    "\t\"\"\"\n",
    "\ttokenizer = add_special_tokens()\n",
    "\tprint(\"Execution Started...\")\n",
    "\ttrain_ids = []\n",
    "\tfile_id_map = {}\n",
    "\ti = 0\n",
    "\tfor file in file_names:\n",
    "\t\tfile = os.path.join(os.getcwd(),directory,file)\n",
    "\t\twith open(file,'r',encoding='utf-8') as f:\n",
    "\t\t\tlines = f.read().split('\\n\\n')\n",
    "\t\tarticle, abstract = get_art_abs(lines)\n",
    "\t\tarticle, abstract = tokenizer.encode(article), tokenizer.encode(abstract)\n",
    "\t\tif len(article)>0 and len(abstract)>0 and (len(article)+len(abstract))<=1023:\n",
    "\t\t\ttrain_ids.append(i)\n",
    "\t\t\twrite_json(i,article,abstract)\n",
    "\t\t\tfile_id_map[i] = os.path.basename(file).replace('.story', '')\n",
    "\t\t\ti += 1\n",
    "\t\t\tif i%100==0:\n",
    "\t\t\t\tprint(i, \" files written\")\n",
    "\n",
    "\n",
    "\tx,y = int(len(train_ids)*0.8), int(len(train_ids)*0.9)\n",
    "\tvalid_ids = train_ids[x:y]\n",
    "\ttest_ids = train_ids[y:]\n",
    "\ttrain_ids = train_ids[:x]\n",
    "\twith open(\"ids.json\",'w') as f:\n",
    "\t\tjs = {}\n",
    "\t\tjs['train_ids'] = train_ids\n",
    "\t\tjs['valid_ids'] = valid_ids\n",
    "\t\tjs['test_ids'] = test_ids\n",
    "\t\tjson.dump(js,f)\n",
    "\n",
    "\t# file_id_map maps the json file ids to actual cnn/dm file names ending with \".story\"\n",
    "\tprint(\"saving file_id_map...\")\n",
    "\twith open(\"file_id_map.pickle\", 'wb') as f:\n",
    "\t\tpickle.dump(file_id_map,f)\n",
    "\tprint(\"file_id_map saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d30c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\tstart = time.time()\n",
    "\twith open(sys.argv[1],'rb') as f:\n",
    "\t\tfile_sizes = pickle.load(f)\n",
    "\tfile_names = [file for file,size in file_sizes.items() if size<=1023] #only consider files with total no of tokens less than 1024\n",
    "\tif sys.argv[1].startswith(\"cnn\"):\n",
    "\t\tdirectory = \"cnn_stories_tokenized\"\n",
    "\t\tos.chdir('./CNN/')\n",
    "\telse:\n",
    "\t\tdirectory = \"dm_stories_tokenized\"\n",
    "\t\tos.chdir('./DM/')\n",
    "\t# directory = \"gpt2_1024_data\"\n",
    "\t# os.chdir('./CNN/')\n",
    "\tmain(file_names, directory)\n",
    "\tprint(\"total_time_taken: \", (time.time()-start)/60, \" minutes\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
